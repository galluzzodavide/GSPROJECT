{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2fdf7e3",
   "metadata": {},
   "source": [
    "_preprocess stage_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f08c1a",
   "metadata": {},
   "source": [
    "Encoder fonde dati eterogenei (satellite, modelli, sensori) per produrre uno stato atmosferico\n",
    "iniziale coerente su una griglia uniforme.\n",
    "\n",
    "NOAA APT Satellite Images: These are low-earth-orbit weather satellite images (e.g. from\n",
    " NOAA-15/18) with ~4 km/pixel resolution, typically two channels (infrared and visible) broadcast\n",
    " via Automatic Picture Transmission.\n",
    "\n",
    "Open Oceanographic/Meteorological API Data: This includes buoy measurements (e.g. wave\n",
    " height, sea temperature), weather radar snapshots (e.g. precipitation radar images), and ship\n",
    " observations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d11d29",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------\n",
    "All data must be mapped to a common geospatial\n",
    " grid. Define a latitude-longitude grid covering the Western Mediterranean at the desired\n",
    " resolution (1 km × 1 km).\n",
    "\n",
    "Interpolate point data (buoys, ship observations) onto this grid. A simple approach is nearest\n",
    "neighbor or inverse-distance weighting for each variable. Satellite and radar data, which may\n",
    " come as images or their own grids, should be reprojected or resampled onto the common grid.\n",
    "\n",
    "Normalization: Once all data layers are on the same grid, stack them into a multi-channel array.\n",
    "Normalize each channel (e.g. min-max scaling or z-score) so that different data ranges are comparable for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b956d",
   "metadata": {},
   "outputs": [],
   "source": [
    " import requests, json\n",
    " from PIL import Image\n",
    " import numpy as np\n",
    " # Define grid parameters (example bounds and resolution)\n",
    " \n",
    " lat_min, lat_max = 30.0, 45.0\n",
    " lon_min, lon_max =-5.0, 15.0\n",
    " res_deg = 0.01 # ~1 km\n",
    " lat_vals = np.arange(lat_min, lat_max, res_deg)\n",
    " lon_vals = np.arange(lon_min, lon_max, res_deg)\n",
    " NY, NX = len(lat_vals), len(lon_vals)\n",
    " \n",
    " # Initialize multichannel grid (e.g. 6 channels)\n",
    " data_grid = np.zeros((6, NY, NX), dtype=np.float32)\n",
    " \n",
    " # 1. Fetch latest NOAA APT satellite image (simulate by loading from file or URL)\n",
    " sat_img = Image.open(\"noaa_apt_latest.jpg\") # placeholder for actual fetch\n",
    " sat_arr = np.array(sat_img.convert(\"L\")) # convert to grayscale numpy array\n",
    "\n",
    " # Resample satellite image to grid resolution:\n",
    " # (In practice, use PIL resize or cv2.remap for projection. Here assume already aligned.)\n",
    " sat_arr_resized = np.array(sat_img.resize((NX, NY)))\n",
    " data_grid[0,:,:] = sat_arr_resized # e.g. IR channel\n",
    "\n",
    " # If APT has a second channel (e.g. visible), place it in data_grid[1]\n",
    "\n",
    " # 2. Fetch buoy data from an open API (e.g. NOAA NDBC or Copernicus Marine)\n",
    "\n",
    " buoy_url = \"https://api.example.com/latest_buoys?region=med\"\n",
    " resp = requests.get(buoy_url)\n",
    " buoy_data = resp.json() # assume JSON with list of {lat, lon, value} for some variable\n",
    " for buoy in buoy_data[\"stations\"]:\n",
    " lat, lon = buoy[\"latitude\"], buoy[\"longitude\"]\n",
    " val = buoy[\"measurement\"][\"value\"]\n",
    " # Find nearest grid index\n",
    " iy = int((lat-lat_min) / res_deg)\n",
    " ix = int((lon-lon_min) / res_deg)\n",
    " if 0 <= iy < NY and 0 <= ix < NX:\n",
    " data_grid[2, iy, ix] = val # e.g. assign sea surface temp at buoy location\n",
    "\n",
    " # 3. Fetch radar data (e.g. rainfall radar composite)\n",
    " radar_img_data = requests.get(\"https://openapi.example.com/radar/med_latest.png\").content\n",
    " with open(\"radar.png\", \"wb\") as f:\n",
    " f.write(radar_img_data)\n",
    " radar_img = Image.open(\"radar.png\")\n",
    " radar_arr = np.array(radar_img.convert(\"L\"))\n",
    " # Assume radar image already in lat/lon projection for region; resample to grid\n",
    " radar_arr_resized = np.array(radar_img.resize((NX, NY)))\n",
    " data_grid[3,:,:] = radar_arr_resized # e.g. rainfall intensity\n",
    "\n",
    " # 4. Fetch ship observations (if available) similar to buoy\n",
    " ship_url = \"https://api.example.com/ships/meteo\"\n",
    " ship_data = requests.get(ship_url).json()\n",
    " for obs in ship_data[\"observations\"]:\n",
    " lat, lon = obs[\"lat\"], obs[\"lon\"]\n",
    " wind = obs[\"wind_speed\"]\n",
    " iy = int((lat- lat_min) / res_deg); ix = int((lon- lon_min) / res_deg)\n",
    " if 0 <= iy < NY and 0 <= ix < NX:\n",
    " data_grid[4, iy, ix] = wind # place ship-reported wind speed\n",
    "\n",
    " # 5. Interpolate missing values on the grid if necessary:\n",
    " # For simplicity, fill small gaps via nearest neighbor\n",
    " for ch in range(data_grid.shape[0]):\n",
    " layer = data_grid[ch]\n",
    " mask = (layer == 0) # assuming 0 means no data; in practice use NaN\n",
    " if mask.any():\n",
    " # simple nearest neighbor fill\n",
    " coords = np.array(np.nonzero(~mask)).T\n",
    " vals = layer[~mask]\n",
    " from scipy import spatial\n",
    " tree = spatial.KDTree(coords)\n",
    " missing_coords = np.array(np.nonzero(mask)).T\n",
    " nearest = tree.query(missing_coords)[1]\n",
    " layer[mask] = vals[nearest]\n",
    " data_grid[ch] = layer\n",
    "\n",
    " # 6. Normalization (e.g., scale each channel 0-1)\n",
    " for ch in range(data_grid.shape[0]):\n",
    " arr = data_grid[ch]\n",
    " data_grid[ch] = (arr- np.nanmin(arr)) / (np.nanmax(arr)- np.nanmin(arr)\n",
    " + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e04456",
   "metadata": {},
   "source": [
    "_encoder_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c571c",
   "metadata": {},
   "source": [
    "The Encoder’s job is to ingest the multi-modal, multi-channel grid (and implicitly the point data, since we\n",
    " have interpolated them onto the grid) and produce a learned representation (embedding) that captures\n",
    " the current state of the atmosphere/ocean. We choose a Vision Transformer (ViT) for this task because\n",
    " ViTs use self-attention to capture global spatial relationships, which can be advantageous for\n",
    " integrating data over a large region.\n",
    "\n",
    "ViT treats the image as a sequence of patches and models global interactions between all patches . Each input image\n",
    "(our multi-channel grid) is split into patches, flattened, and linearly projected into an embedding space.\n",
    "\n",
    "Positional embeddings are added to preserve spatial context, then a Transformer encoder (multi\n",
    "head self-attention layers) processes the sequence of patch embeddings . We take the Transformer’s\n",
    " output as the encoded state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa114f",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "Implementation details: We need to adjust a standard ViT to handle our input. Typically, ViT expects a 3\n",
    "channel RGB image; here we have N-channel data (say 6 channels as in our example). We implement a\n",
    " patch embedding layer as a Conv2d with kernel size = patch size and output channels = \n",
    "embed_dim\n",
    " (the transformer dimension). This will reduce each patch (of size e.g. 16×16) across all input channels\n",
    " into a vector. \n",
    "\n",
    "\n",
    " In our design, we will output a spatial feature map that can interface\n",
    " with the Processor. One approach is to reshape the Transformer output (excluding the class token) back\n",
    " to a 2D grid of patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e315aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class ViTEncoder(nn.Module):\n",
    "def __init__(self, img_size, patch_size, in_channels, embed_dim, num_layers=6, num_heads=8):\n",
    "    super().__init__()\n",
    "    assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "    self.patch_size = patch_size\n",
    "    self.num_patches = (img_size // patch_size) ** 2\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    # Patch embedding: conv layer that produces embed_dim feature maps from input channels\n",
    "    self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    # Class token and positional embedding\n",
    "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "    self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    " \n",
    "    # Transformer Encoder\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=4*embed_dim)\n",
    "    self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "    \n",
    "def forward(self, x):\n",
    "    # x shape: (B, in_channels, H, W)\n",
    "    B = x.size(0)\n",
    "    # Create patch embeddings\n",
    "    patches = self.patch_embed(x)\n",
    "    # (B, embed_dim, H/patch, W/ patch)\n",
    "\n",
    "    patches = patches.flatten(2).transpose(1, 2) # (B, N, embed_dim), \n",
    "    N=num_patches\n",
    "    \n",
    "    # Prepend class token\n",
    "    cls_tokens = self.cls_token.expand(B,-1,-1) # (B, 1, embed_dim)\n",
    "    tokens = torch.cat([cls_tokens, patches], dim=1) # (B, N+1, embed_dim)\n",
    "    tokens = tokens + self.pos_embed[:, :tokens.size(1), :]\n",
    " \n",
    "    # Transformer encoding\n",
    "    tokens = tokens.transpose(0, 1)\n",
    "    # (N+1, B, embed_dim) for transformer\n",
    "    enc_outputs = self.transformer(tokens)\n",
    "    enc_outputs = enc_outputs.transpose(0, 1)\n",
    "    # Separate class token and patch embeddings\n",
    "    cls_out = enc_outputs[:, 0, :]\n",
    "    # (N+1, B, embed_dim)\n",
    "    # (B, N+1, embed_dim)\n",
    "    # (B, embed_dim)\n",
    " patch_out = enc_outputs[:, 1:, :].transpose(1, 2) # (B, embed_dim, N)\n",
    " \n",
    " # Reshape patch_out back to spatial grid\n",
    " grid_size = int(self.num_patches**0.5)\n",
    " feature_map = patch_out.view(B, self.embed_dim, grid_size, grid_size)\n",
    " \n",
    " # (B, embed_dim, H/patch, W/patch)\n",
    " return cls_out, feature_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d042d8be",
   "metadata": {},
   "source": [
    "This encoder produces two outputs: \n",
    "cls_out (a global feature vector summarizing the state) and\n",
    " feature_map (a smaller spatial feature map of size \n",
    "H/patch × W/patch capturing spatial details).\n",
    " We will use \n",
    "feature_map as input to the next stage, and potentially use \n",
    "cls_out to initialize hidden\n",
    " states."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
